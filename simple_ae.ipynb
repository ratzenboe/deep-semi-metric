{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-24T02:58:23.767791Z",
     "start_time": "2025-04-24T02:58:23.765622Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from modules import Trafo\n",
    "from modules.galactic2icrs import cartesianGalactic_to_ICRS_torch\n",
    "from modules import pairwise_min_velocity_diff_torch"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create data",
   "id": "ea2cedb38d474652"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T02:58:46.394287Z",
     "start_time": "2025-04-24T02:58:46.369027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = 10_000\n",
    "mu = np.array([\n",
    "    # X  Y  Z\n",
    "    100, -10, 5,\n",
    "    # U V  W\n",
    "    1, 2, -1\n",
    "])\n",
    "# Dispersion in X\n",
    "sx = 1\n",
    "# Velocity dispersion\n",
    "sv = 1\n",
    "C = np.diag([sx, sx, sx, sv, sv, sv])\n",
    "\n",
    "# This is the Cartesian space where we should cluster in --> the latent space learned by the AE\n",
    "data = np.random.multivariate_normal(mu, C, N)\n",
    "data_icrs = Trafo().cart2spher(data)"
   ],
   "id": "3e95e1597cc8a0d3",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T02:18:58.528035Z",
     "start_time": "2025-04-24T02:18:58.525428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RandomSubset5DDataset(IterableDataset):\n",
    "    def __init__(self, data5D: torch.Tensor, subset_size: int, iters_per_epoch: int):\n",
    "        \"\"\"\n",
    "        data5D:        (N, 5) tensor of your full catalogue [ra, dec, plx, pmra, pmdec]\n",
    "        subset_size:   M, how many samples per batch\n",
    "        iters_per_epoch: number of random subsets to draw each epoch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = data5D\n",
    "        self.N = data5D.size(0)\n",
    "        self.M = subset_size\n",
    "        self.iters = iters_per_epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.iters):\n",
    "            # sample M unique indices without replacement\n",
    "            idx = torch.randperm(self.N, device=self.data.device)[:self.M]\n",
    "            yield self.data[idx]  # shape (M, 5)\n",
    "\n",
    "# ——————————————\n",
    "# Usage example:\n",
    "\n",
    "# 1) your full 5D dataset\n",
    "full5D = torch.as_tensor(data_icrs[['ra', 'dec', 'parallax', 'pmra', 'pmdec']].values, dtype=torch.float32)\n",
    "\n",
    "# 2) make the IterableDataset\n",
    "M = 512    # samples per batch\n",
    "Iters = 200    # subsets per epoch\n",
    "ds = RandomSubset5DDataset(full5D, M, Iters)\n",
    "\n",
    "# 3) wrap in a DataLoader (batch_size=None means each yield is one “batch”)\n",
    "loader  = DataLoader(ds, batch_size=None)"
   ],
   "id": "bf861a675e7b1f34",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create encoder network",
   "id": "9e162051c2e49e48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T03:29:57.238633Z",
     "start_time": "2025-04-24T03:29:57.232726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_mlp(input_dim: int, hidden_dims: tuple, output_dim: int,\n",
    "              activation=nn.ReLU, inplace=True) -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    Builds an MLP from input_dim → *hidden_dims → output_dim*,\n",
    "    inserting activation() between each linear layer.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    dims = [input_dim, *hidden_dims, output_dim]\n",
    "    for i in range(len(dims) - 1):\n",
    "        layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "        # add activation after every layer except the last\n",
    "        if i < len(dims) - 2:\n",
    "            layers.append(activation(inplace=inplace))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class SemiAnalyticAutoencoder(nn.Module):\n",
    "    def __init__(self, M, hidden_dims=(128,64)):\n",
    "        \"\"\"\n",
    "        M : int\n",
    "            number of samples in each random batch (so diff-matrices are M×M)\n",
    "        hidden_dims : tuple\n",
    "            sizes of the two hidden layers in the row-wise MLP encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "\n",
    "        # dynamically build encoder: M → *hidden_dims → 6\n",
    "        self.encoder =build_mlp(\n",
    "            input_dim=M,\n",
    "            hidden_dims=hidden_dims,\n",
    "            output_dim=6\n",
    "        )\n",
    "\n",
    "    def forward(self, batch5D):\n",
    "        \"\"\"\n",
    "        batch5D : Tensor of shape (B, M, 5)\n",
    "            last-dim = [ra, dec, parallax, pmra, pmdec]\n",
    "        Returns\n",
    "        -------\n",
    "        recon        : Tensor (B, M, M)\n",
    "        diff_target  : Tensor (B, M, M)\n",
    "        \"\"\"\n",
    "        x5 = torch.as_tensor(batch5D)\n",
    "        if x5.dim() != 3 or x5.size(-1) != 5:\n",
    "            raise ValueError(f\"Expected (B, {self.M}, 5), got {tuple(x5.shape)}\")\n",
    "        B, M, _ = x5.shape\n",
    "        if M != self.M:\n",
    "            raise ValueError(f\"Model built for M={self.M}, got batch size {M}\")\n",
    "\n",
    "        # 1) analytic ground truth\n",
    "        #    pairwise_min_velocity_diff_torch ignores the 6th dim, so 5 dims is fine\n",
    "        diff_target = pairwise_min_velocity_diff_torch(x5)  # (B, M, M)\n",
    "\n",
    "        # 2) encode each row of diff_target → 6D\n",
    "        rows = diff_target.reshape(B * M, M)  # (B*M, M)\n",
    "        lat  = self.encoder(rows)             # (B*M, 6)\n",
    "        lat  = lat.view(B, M, 6)              # (B, M, 6)\n",
    "\n",
    "        # 3) analytic decode back to M×M\n",
    "        astro_icrs = cartesianGalactic_to_ICRS_torch(lat)         # (B, M, 6)\n",
    "        # keep the first 5 dims to regress back to x5\n",
    "        astro_pred5 = astro_icrs[..., :5]                         # (B, M, 5)\n",
    "        recon = pairwise_min_velocity_diff_torch(astro_icrs)      # (B, M, M)\n",
    "\n",
    "        return recon, diff_target, astro_pred5"
   ],
   "id": "6564fafbe0fc8eee",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:29:04.492006Z",
     "start_time": "2025-04-24T03:30:29.522367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SemiAnalyticAutoencoder(M=512, hidden_dims=(1024, 512, 256, 128, 64, 32)).to('cpu')\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 500\n",
    "beta = 0.1\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch5D in loader:           # each batch5D: (M,5)\n",
    "        batch5D = batch5D.to('cpu').unsqueeze(0)  # → (1, M, 5) or stack multiple\n",
    "\n",
    "        recon, target, astro_pred5 = model(batch5D)            # both (1, M, M)\n",
    "        # 1) pairwise‐matrix MSE\n",
    "        loss_mat = F.mse_loss(recon, target)\n",
    "        # 2) astro‐features MSE\n",
    "        # compare astro_pred5 against original x5\n",
    "        loss_astro = F.mse_loss(astro_pred5, batch5D)\n",
    "        # total loss\n",
    "        loss = loss_mat + beta * loss_astro\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}: avg loss = {total_loss/Iters:.6f}\")"
   ],
   "id": "fc956d8706c6c6f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: avg loss = 7276.502719\n",
      "Epoch 1: avg loss = 1.201085\n",
      "Epoch 2: avg loss = 1.198250\n",
      "Epoch 3: avg loss = 1.178690\n",
      "Epoch 4: avg loss = 1.144219\n",
      "Epoch 5: avg loss = 1.118202\n",
      "Epoch 6: avg loss = 1.116410\n",
      "Epoch 7: avg loss = 1.121658\n",
      "Epoch 8: avg loss = 1.118961\n",
      "Epoch 9: avg loss = 1.114386\n",
      "Epoch 10: avg loss = 1.114094\n",
      "Epoch 11: avg loss = 1.115171\n",
      "Epoch 12: avg loss = 1.115516\n",
      "Epoch 13: avg loss = 1.120365\n",
      "Epoch 14: avg loss = 1.112367\n",
      "Epoch 15: avg loss = 1.114928\n",
      "Epoch 16: avg loss = 1.111764\n",
      "Epoch 17: avg loss = 1.114697\n",
      "Epoch 18: avg loss = 1.114460\n",
      "Epoch 19: avg loss = 1.114372\n",
      "Epoch 20: avg loss = 1.110418\n",
      "Epoch 21: avg loss = 1.105784\n",
      "Epoch 22: avg loss = 1.112582\n",
      "Epoch 23: avg loss = 1.111187\n",
      "Epoch 24: avg loss = 1.109915\n",
      "Epoch 25: avg loss = 1.109958\n",
      "Epoch 26: avg loss = 1.102282\n",
      "Epoch 27: avg loss = 1.108710\n",
      "Epoch 28: avg loss = 1.107262\n",
      "Epoch 29: avg loss = 1.097012\n",
      "Epoch 30: avg loss = 1.087760\n",
      "Epoch 31: avg loss = 1.067882\n",
      "Epoch 32: avg loss = 1.034269\n",
      "Epoch 33: avg loss = 1.024962\n",
      "Epoch 34: avg loss = 0.995176\n",
      "Epoch 35: avg loss = 0.999525\n",
      "Epoch 36: avg loss = 0.992260\n",
      "Epoch 37: avg loss = 0.995636\n",
      "Epoch 38: avg loss = 0.996256\n",
      "Epoch 39: avg loss = 0.994023\n",
      "Epoch 40: avg loss = 1.000002\n",
      "Epoch 41: avg loss = 0.994389\n",
      "Epoch 42: avg loss = 0.985778\n",
      "Epoch 43: avg loss = 1.000073\n",
      "Epoch 44: avg loss = 0.976732\n",
      "Epoch 45: avg loss = 0.991207\n",
      "Epoch 46: avg loss = 0.987571\n",
      "Epoch 47: avg loss = 0.985631\n",
      "Epoch 48: avg loss = 0.978341\n",
      "Epoch 49: avg loss = 1.058054\n",
      "Epoch 50: avg loss = 0.985632\n",
      "Epoch 51: avg loss = 0.999804\n",
      "Epoch 52: avg loss = 0.994975\n",
      "Epoch 53: avg loss = 0.991031\n",
      "Epoch 54: avg loss = 0.980076\n",
      "Epoch 55: avg loss = 0.988311\n",
      "Epoch 56: avg loss = 0.984458\n",
      "Epoch 57: avg loss = 0.995198\n",
      "Epoch 58: avg loss = 0.991090\n",
      "Epoch 59: avg loss = 0.986779\n",
      "Epoch 60: avg loss = 0.990772\n",
      "Epoch 61: avg loss = 0.994259\n",
      "Epoch 62: avg loss = 0.977358\n",
      "Epoch 63: avg loss = 0.973200\n",
      "Epoch 64: avg loss = 0.987375\n",
      "Epoch 65: avg loss = 0.979809\n",
      "Epoch 66: avg loss = 1.008792\n",
      "Epoch 67: avg loss = 0.966200\n",
      "Epoch 68: avg loss = 0.979758\n",
      "Epoch 69: avg loss = 0.968804\n",
      "Epoch 70: avg loss = 0.975681\n",
      "Epoch 71: avg loss = 0.963803\n",
      "Epoch 72: avg loss = 0.983884\n",
      "Epoch 73: avg loss = 0.972537\n",
      "Epoch 74: avg loss = 0.979068\n",
      "Epoch 75: avg loss = 0.976240\n",
      "Epoch 76: avg loss = 0.968804\n",
      "Epoch 77: avg loss = 0.966277\n",
      "Epoch 78: avg loss = 0.972325\n",
      "Epoch 79: avg loss = 0.968333\n",
      "Epoch 80: avg loss = 0.953816\n",
      "Epoch 81: avg loss = 0.965180\n",
      "Epoch 82: avg loss = 0.964045\n",
      "Epoch 83: avg loss = 0.943741\n",
      "Epoch 84: avg loss = 0.948135\n",
      "Epoch 85: avg loss = 0.953812\n",
      "Epoch 86: avg loss = 0.949757\n",
      "Epoch 87: avg loss = 0.946241\n",
      "Epoch 88: avg loss = 0.949609\n",
      "Epoch 89: avg loss = 0.936562\n",
      "Epoch 90: avg loss = 0.940404\n",
      "Epoch 91: avg loss = 0.938084\n",
      "Epoch 92: avg loss = 0.942861\n",
      "Epoch 93: avg loss = 0.941978\n",
      "Epoch 94: avg loss = 0.951674\n",
      "Epoch 95: avg loss = 0.948525\n",
      "Epoch 96: avg loss = 0.920996\n",
      "Epoch 97: avg loss = 0.922624\n",
      "Epoch 98: avg loss = 0.932487\n",
      "Epoch 99: avg loss = 0.924591\n",
      "Epoch 100: avg loss = 0.924523\n",
      "Epoch 101: avg loss = 0.931463\n",
      "Epoch 102: avg loss = 0.918136\n",
      "Epoch 103: avg loss = 0.924701\n",
      "Epoch 104: avg loss = 0.921238\n",
      "Epoch 105: avg loss = 0.922916\n",
      "Epoch 106: avg loss = 0.913432\n",
      "Epoch 107: avg loss = 0.905783\n",
      "Epoch 108: avg loss = 0.900755\n",
      "Epoch 109: avg loss = 0.902163\n",
      "Epoch 110: avg loss = 0.905745\n",
      "Epoch 111: avg loss = 0.892433\n",
      "Epoch 112: avg loss = 0.886234\n",
      "Epoch 113: avg loss = 0.888234\n",
      "Epoch 114: avg loss = 0.892954\n",
      "Epoch 115: avg loss = 0.894161\n",
      "Epoch 116: avg loss = 0.877343\n",
      "Epoch 117: avg loss = 0.880146\n",
      "Epoch 118: avg loss = 0.889170\n",
      "Epoch 119: avg loss = 0.890503\n",
      "Epoch 120: avg loss = 0.869104\n",
      "Epoch 121: avg loss = 0.871559\n",
      "Epoch 122: avg loss = 0.879047\n",
      "Epoch 123: avg loss = 0.880508\n",
      "Epoch 124: avg loss = 0.873159\n",
      "Epoch 125: avg loss = 0.868296\n",
      "Epoch 126: avg loss = 0.875879\n",
      "Epoch 127: avg loss = 0.852719\n",
      "Epoch 128: avg loss = 0.869257\n",
      "Epoch 129: avg loss = 0.862359\n",
      "Epoch 130: avg loss = 0.869871\n",
      "Epoch 131: avg loss = 0.876499\n",
      "Epoch 132: avg loss = 0.871951\n",
      "Epoch 133: avg loss = 0.871814\n",
      "Epoch 134: avg loss = 0.866484\n",
      "Epoch 135: avg loss = 0.857237\n",
      "Epoch 136: avg loss = 0.868768\n",
      "Epoch 137: avg loss = 0.857771\n",
      "Epoch 138: avg loss = 0.874656\n",
      "Epoch 139: avg loss = 0.864016\n",
      "Epoch 140: avg loss = 0.865327\n",
      "Epoch 141: avg loss = 0.864865\n",
      "Epoch 142: avg loss = 0.861394\n",
      "Epoch 143: avg loss = 0.870037\n",
      "Epoch 144: avg loss = 0.865517\n",
      "Epoch 145: avg loss = 0.856395\n",
      "Epoch 146: avg loss = 0.866787\n",
      "Epoch 147: avg loss = 0.867693\n",
      "Epoch 148: avg loss = 0.867210\n",
      "Epoch 149: avg loss = 0.870834\n",
      "Epoch 150: avg loss = 0.856936\n",
      "Epoch 151: avg loss = 0.861104\n",
      "Epoch 152: avg loss = 0.864011\n",
      "Epoch 153: avg loss = 0.863891\n",
      "Epoch 154: avg loss = 0.861503\n",
      "Epoch 155: avg loss = 0.857685\n",
      "Epoch 156: avg loss = 0.865893\n",
      "Epoch 157: avg loss = 0.859778\n",
      "Epoch 158: avg loss = 0.864960\n",
      "Epoch 159: avg loss = 0.854809\n",
      "Epoch 160: avg loss = 0.861882\n",
      "Epoch 161: avg loss = 0.859440\n",
      "Epoch 162: avg loss = 0.854241\n",
      "Epoch 163: avg loss = 0.854874\n",
      "Epoch 164: avg loss = 0.862240\n",
      "Epoch 165: avg loss = 0.849956\n",
      "Epoch 166: avg loss = 0.853024\n",
      "Epoch 167: avg loss = 0.862440\n",
      "Epoch 168: avg loss = 0.857797\n",
      "Epoch 169: avg loss = 0.850737\n",
      "Epoch 170: avg loss = 0.866366\n",
      "Epoch 171: avg loss = 0.862458\n",
      "Epoch 172: avg loss = 0.863034\n",
      "Epoch 173: avg loss = 0.848836\n",
      "Epoch 174: avg loss = 0.849766\n",
      "Epoch 175: avg loss = 0.861503\n",
      "Epoch 176: avg loss = 0.856266\n",
      "Epoch 177: avg loss = 0.866742\n",
      "Epoch 178: avg loss = 0.858396\n",
      "Epoch 179: avg loss = 0.856344\n",
      "Epoch 180: avg loss = 0.853442\n",
      "Epoch 181: avg loss = 0.862688\n",
      "Epoch 182: avg loss = 0.852506\n",
      "Epoch 183: avg loss = 0.856699\n",
      "Epoch 184: avg loss = 0.855619\n",
      "Epoch 185: avg loss = 0.855227\n",
      "Epoch 186: avg loss = 0.859122\n",
      "Epoch 187: avg loss = 0.854985\n",
      "Epoch 188: avg loss = 0.850317\n",
      "Epoch 189: avg loss = 0.850676\n",
      "Epoch 190: avg loss = 0.857318\n",
      "Epoch 191: avg loss = 0.859874\n",
      "Epoch 192: avg loss = 0.847838\n",
      "Epoch 193: avg loss = 0.850095\n",
      "Epoch 194: avg loss = 0.862454\n",
      "Epoch 195: avg loss = 0.853058\n",
      "Epoch 196: avg loss = 0.850458\n",
      "Epoch 197: avg loss = 0.851872\n",
      "Epoch 198: avg loss = 0.853749\n",
      "Epoch 199: avg loss = 0.858091\n",
      "Epoch 200: avg loss = 0.853195\n",
      "Epoch 201: avg loss = 0.855081\n",
      "Epoch 202: avg loss = 0.851458\n",
      "Epoch 203: avg loss = 0.843362\n",
      "Epoch 204: avg loss = 0.858756\n",
      "Epoch 205: avg loss = 0.852562\n",
      "Epoch 206: avg loss = 0.854237\n",
      "Epoch 207: avg loss = 0.844900\n",
      "Epoch 208: avg loss = 0.855744\n",
      "Epoch 209: avg loss = 0.861423\n",
      "Epoch 210: avg loss = 0.862179\n",
      "Epoch 211: avg loss = 0.858199\n",
      "Epoch 212: avg loss = 0.847180\n",
      "Epoch 213: avg loss = 0.848704\n",
      "Epoch 214: avg loss = 0.847722\n",
      "Epoch 215: avg loss = 0.848090\n",
      "Epoch 216: avg loss = 0.851442\n",
      "Epoch 217: avg loss = 0.853712\n",
      "Epoch 218: avg loss = 0.854899\n",
      "Epoch 219: avg loss = 0.862209\n",
      "Epoch 220: avg loss = 0.849616\n",
      "Epoch 221: avg loss = 0.855901\n",
      "Epoch 222: avg loss = 0.856926\n",
      "Epoch 223: avg loss = 0.848072\n",
      "Epoch 224: avg loss = 0.841164\n",
      "Epoch 225: avg loss = 0.848982\n",
      "Epoch 226: avg loss = 0.857295\n",
      "Epoch 227: avg loss = 0.851888\n",
      "Epoch 228: avg loss = 0.857376\n",
      "Epoch 229: avg loss = 0.858821\n",
      "Epoch 230: avg loss = 0.854759\n",
      "Epoch 231: avg loss = 0.844090\n",
      "Epoch 232: avg loss = 0.852638\n",
      "Epoch 233: avg loss = 0.859468\n",
      "Epoch 234: avg loss = 0.857720\n",
      "Epoch 235: avg loss = 0.849751\n",
      "Epoch 236: avg loss = 0.861583\n",
      "Epoch 237: avg loss = 0.855417\n",
      "Epoch 238: avg loss = 0.847910\n",
      "Epoch 239: avg loss = 0.855930\n",
      "Epoch 240: avg loss = 0.844612\n",
      "Epoch 241: avg loss = 0.850345\n",
      "Epoch 242: avg loss = 0.842164\n",
      "Epoch 243: avg loss = 0.857488\n",
      "Epoch 244: avg loss = 0.849934\n",
      "Epoch 245: avg loss = 0.855641\n",
      "Epoch 246: avg loss = 0.849350\n",
      "Epoch 247: avg loss = 0.852855\n",
      "Epoch 248: avg loss = 0.853678\n",
      "Epoch 249: avg loss = 0.851533\n",
      "Epoch 250: avg loss = 0.846197\n",
      "Epoch 251: avg loss = 0.846919\n",
      "Epoch 252: avg loss = 0.856320\n",
      "Epoch 253: avg loss = 0.844678\n",
      "Epoch 254: avg loss = 0.853907\n",
      "Epoch 255: avg loss = 0.861097\n",
      "Epoch 256: avg loss = 0.845126\n",
      "Epoch 257: avg loss = 0.861882\n",
      "Epoch 258: avg loss = 0.858397\n",
      "Epoch 259: avg loss = 0.864218\n",
      "Epoch 260: avg loss = 0.843634\n",
      "Epoch 261: avg loss = 0.851048\n",
      "Epoch 262: avg loss = 0.852177\n",
      "Epoch 263: avg loss = 0.855002\n",
      "Epoch 264: avg loss = 0.851560\n",
      "Epoch 265: avg loss = 0.852333\n",
      "Epoch 266: avg loss = 0.855704\n",
      "Epoch 267: avg loss = 0.848135\n",
      "Epoch 268: avg loss = 0.852369\n",
      "Epoch 269: avg loss = 0.859675\n",
      "Epoch 270: avg loss = 0.847992\n",
      "Epoch 271: avg loss = 0.858854\n",
      "Epoch 272: avg loss = 0.844530\n",
      "Epoch 273: avg loss = 0.859169\n",
      "Epoch 274: avg loss = 0.849969\n",
      "Epoch 275: avg loss = 0.844907\n",
      "Epoch 276: avg loss = 0.852383\n",
      "Epoch 277: avg loss = 0.849772\n",
      "Epoch 278: avg loss = 0.856683\n",
      "Epoch 279: avg loss = 0.842879\n",
      "Epoch 280: avg loss = 0.846084\n",
      "Epoch 281: avg loss = 0.844674\n",
      "Epoch 282: avg loss = 0.851543\n",
      "Epoch 283: avg loss = 0.846228\n",
      "Epoch 284: avg loss = 0.857650\n",
      "Epoch 285: avg loss = 0.847331\n",
      "Epoch 286: avg loss = 0.843119\n",
      "Epoch 287: avg loss = 0.858341\n",
      "Epoch 288: avg loss = 0.846581\n",
      "Epoch 289: avg loss = 0.850996\n",
      "Epoch 290: avg loss = 0.851803\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_mat \u001B[38;5;241m+\u001B[39m beta \u001B[38;5;241m*\u001B[39m loss_astro\n\u001B[1;32m     20\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 21\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     24\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/anaconda3/envs/sigma/lib/python3.12/site-packages/torch/_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    647\u001B[0m     )\n\u001B[0;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sigma/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sigma/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f9f4dbb9f65f6cc8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
